{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reV0us/20009100841/blob/main/deep21_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL9qQvzFPXcT"
      },
      "source": [
        "# a self-contained deep21 tutorial\n",
        "\n",
        "![unet-diagram](https://raw.githubusercontent.com/tlmakinen/deep21/master/tutorial/panels-white.gif)\n",
        "$\\quad$\n",
        "\n",
        "\n",
        "[T. Lucas Makinen](https://orcid.org/0000-0002-3795-6933) (this notebook), [Lachlan T. Lancaster](https://orcid.org/0000-0002-0041-4356), [Francisco Villaescusa-Navarro](https://orcid.org/0000-0002-4816-0455), [Peter Melchior](), Laurence Perrault-Levasseur, Shirley Ho & David Spergel\n",
        "\n",
        "$\\quad$\n",
        "![unet-diagram](https://raw.githubusercontent.com/tlmakinen/deep21/master/tutorial/unet-diagram.png)\n",
        "$\\quad$\n",
        "\n",
        "Here we'll walk through the use of the [deep21 UNet module](https://github.com/tlmakinen/deep21/) created for foreground-cosmological signal separation for 21cm cosmology. We've simplified the approach quite a bit, but hope that this tutorial (geared towards an upper undergraduate-level audience) will provide guidance for use in either 21cm or other high-dimensional image processing tasks. For a pop overview, see this [Twitter thread](https://twitter.com/LucasMakinen/status/1325871656745771008?s=20)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ2kR5vsogTJ"
      },
      "source": [
        "## set up notebook\n",
        "First we're going to pull in the code from GitHub repository, and install necessary packages onto the Colab platform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuP14_Uep2X4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7d3f16-c316-4132-bf93-162f46b904b2"
      },
      "source": [
        "!git clone https://github.com/tlmakinen/deep21.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'deep21' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72ZEm5GAqaAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853a8d29-c00c-4a5a-9ace-86b4fa93da11"
      },
      "source": [
        "# install healpy dependency\n",
        "!pip install healpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: healpy in /usr/local/lib/python3.10/dist-packages (1.16.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from healpy) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from healpy) (1.23.5)\n",
            "Requirement already satisfied: astropy in /usr/local/lib/python3.10/dist-packages (from healpy) (5.3.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from healpy) (1.11.4)\n",
            "Requirement already satisfied: pyerfa>=2.0 in /usr/local/lib/python3.10/dist-packages (from astropy->healpy) (2.0.1.1)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.10/dist-packages (from astropy->healpy) (6.0.1)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from astropy->healpy) (23.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->healpy) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->healpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->healpy) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->healpy) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->healpy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->healpy) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->healpy) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->healpy) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFRxq-5Rp8w4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5cf002-fc48-481c-9d65-8b87477451bc"
      },
      "source": [
        "cd deep21/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deep21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHe1IMxBqCCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5fb9a96-8752-4718-a69d-65b5be82670b"
      },
      "source": [
        "# import the required Libraries\n",
        "!pip install tensorflow==2.14\n",
        "!pip install tensorflow-addons==0.22.0\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "import healpy as hp\n",
        "import os\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.14 in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-addons==0.22.0 in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons==0.22.0) (23.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons==0.22.0) (2.13.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZg8ejRQS3qC"
      },
      "source": [
        "# plot settings\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.rcParams.update({\n",
        "    \"pgf.texsystem\": \"pdflatex\",\n",
        "    'font.family': 'serif',\n",
        "    'text.usetex': False,\n",
        "    'pgf.rcfonts': False,\n",
        "})\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns; sns.set()\n",
        "%matplotlib inline"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEutdRLz9o2G"
      },
      "source": [
        "# load 21 cm data\n",
        "\n",
        "The data for this tutorial has already been formatted into HEALPix voxels. The dataset shape for both observed and clean cosmological signal is $(N_{\\rm voxels}, N_x, N_y, N_\\nu) = (192,64,64,64)$. To do the PCA preprocessing we'll flatten the data back into HEALPix maps (like those obtained from `.fits` files in the full code). Then the PCA function will rearrange the PCA residual map *back* to voxel shape.\n",
        "\n",
        "$ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\rm foregrounds\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ cosmological\\ signal $\n",
        "\n",
        "$\\quad$\n",
        "![unet-diagram](https://raw.githubusercontent.com/tlmakinen/deep21/master/tutorial/panel-comparison.png)\n",
        "$\\quad$\n",
        "\n",
        "\n",
        "We're going to load 5 full-sky example simulations to demonstrate training and PCA subtraction. We'll pull them in from Zenodo.\n",
        "\n",
        "**Note:** this might take a minute or two (files are $\\sim$2 Gb each)\n",
        "\n",
        "**if you're in a hurry:** uncomment the lines below to download a single simulation from the Zenodo repository. You will not be able to compute power spectrum estimations for and entire predicted sky, but you will be able to make the voxel-wise predictions.\n",
        "\n",
        "___\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OkBpzjoFLmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b81c2eb-12b1-4641-b6ea-31d3f7caf98e"
      },
      "source": [
        "!wget https://zenodo.org/record/4133772/files/cosmo.npy\n",
        "!wget https://zenodo.org/record/4133772/files/obs.npy\n",
        "# !wget https://zenodo.org/record/4133772/files/cosmo-single.npy\n",
        "# !wget https://zenodo.org/record/4133772/files/obs-single.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-16 06:11:17--  https://zenodo.org/record/4133772/files/cosmo.npy\n",
            "Resolving zenodo.org (zenodo.org)... 188.184.98.238, 188.184.103.159, 188.185.79.172, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.184.98.238|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/4133772/files/cosmo.npy [following]\n",
            "--2024-01-16 06:11:18--  https://zenodo.org/records/4133772/files/cosmo.npy\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2013266048 (1.9G) [application/octet-stream]\n",
            "Saving to: ‘cosmo.npy.3’\n",
            "\n",
            "cosmo.npy.3         100%[===================>]   1.88G  17.2MB/s    in 3m 41s  \n",
            "\n",
            "2024-01-16 06:14:59 (8.71 MB/s) - ‘cosmo.npy.3’ saved [2013266048/2013266048]\n",
            "\n",
            "--2024-01-16 06:14:59--  https://zenodo.org/record/4133772/files/obs.npy\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.79.172, 188.184.98.238, 188.184.103.159, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.79.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/4133772/files/obs.npy [following]\n",
            "--2024-01-16 06:14:59--  https://zenodo.org/records/4133772/files/obs.npy\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2013266048 (1.9G) [application/octet-stream]\n",
            "Saving to: ‘obs.npy.3’\n",
            "\n",
            "obs.npy.3             5%[>                   ] 112.18M   607KB/s    eta 51m 40s"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dark6iDEj_ek"
      },
      "source": [
        "index_arr = np.load('./sim_info/rearr_nside4.npy')\n",
        "\n",
        "# read in the observed and cosmological maps -- change to `cosmo-single.npy`\n",
        "# if using smaller dataset\n",
        "cosmo = np.load('cosmo.npy')\n",
        "obs_map = np.load('obs.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSRD4I_0DTOY"
      },
      "source": [
        "# define the (simplified) PCA preprocessing function\n",
        "Here we code a simplified version of the probabilistic PCA subtraction we used to preprocess our maps. We'll remove the first 3 components and use the residual to train our deep network.\n",
        "___\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whlRGZ_BKWj1"
      },
      "source": [
        "index_arr = np.load('./sim_info/rearr_nside4.npy')\n",
        "# ACTUAL FREQUENCY MEASUREMENTS\n",
        "nutable = './sim_info/' + \"nuTable.txt\"\n",
        "(bn,nu_bot,nu_top,z_bot,z_top) = np.loadtxt(nutable).T\n",
        "nu_arr = ((nu_bot + nu_top)/2.)[:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6MkGT8_Dxkq"
      },
      "source": [
        "# first some additional packages that we'll need\n",
        "import healpy as hp\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "def gen_rearr(nside):\n",
        "    # recursive funtion for finding the right\n",
        "    # ordering for the nested pixels\n",
        "    if (nside==1):\n",
        "        return np.array([0,1,2,3])\n",
        "    else:\n",
        "        smaller = np.reshape(gen_rearr(nside-1),(2**(nside-1),2**(nside-1)))\n",
        "        npixsmaller = 2**(2*(nside-1))\n",
        "        top = np.concatenate((smaller,smaller+npixsmaller),axis=1)\n",
        "        bot = np.concatenate((smaller+2*npixsmaller,smaller+3*npixsmaller),axis=1)\n",
        "        whole = np.concatenate((top,bot))\n",
        "        return whole.flatten()\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# for this demo, we'll pull in a pre-computed index to get our data\n",
        "# from the shape (192, N_x, N_y, N_nu) to the HEALPix ring\n",
        "# data format (192*64*64, 64)\n",
        "\n",
        "def pca_subtraction(input_map, n_comp, index_array, n_nu=None,\n",
        "                    nu_arr=None, nu_start=0, n_nu_out=64, n_nu_avg=3):\n",
        "\n",
        "    # \"GLOBAL\" parameters ----------------------------------------------------\n",
        "    MAP_NSIDE = 256\n",
        "    SIM_NSIDE = MAP_NSIDE\n",
        "    WINDOW_NSIDE = 4\n",
        "    NUM_SIMS = 1\n",
        "    # resolution of the outgoing window\n",
        "    NPIX_WINDOW = (MAP_NSIDE/WINDOW_NSIDE)**2\n",
        "    # actual side length of window\n",
        "    WINDOW_LENGTH = int(np.sqrt(NPIX_WINDOW))\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # rearrange indices\n",
        "    rearr = gen_rearr(int(np.log2(MAP_NSIDE/WINDOW_NSIDE)))\n",
        "    nwinds = int(hp.nside2npix(WINDOW_NSIDE))\n",
        "\n",
        "    if nu_arr is not None:\n",
        "        print('working in frequency range ', nu_arr[nu_start], '--',\n",
        "                                nu_arr[nu_start + (n_nu*n_nu_avg)-1],\n",
        "                ' MHz')\n",
        "\n",
        "    # initialize the PCA algorithm\n",
        "    pca = PCA()\n",
        "    # allocate the output array\n",
        "    pca_reduced_out = np.zeros(input_map.shape)\n",
        "\n",
        "    # flatten input map into full-sky maps stacked in frequency\n",
        "    input_map = input_map.reshape((-1, 64))[index_array]\n",
        "\n",
        "    # do PCA removal of n_comp components\n",
        "    pca.fit(input_map)\n",
        "    obs_pca = pca.transform(input_map)\n",
        "    ind_arr = np.reshape(np.arange(np.prod(obs_pca.shape)),obs_pca.shape)\n",
        "\n",
        "\n",
        "    mask = np.ones(obs_pca.shape)\n",
        "    for i in range(n_comp,obs_pca.shape[1]):\n",
        "        mask[ind_arr%obs_pca.shape[1]==i] = 0\n",
        "    obs_pca = obs_pca*mask\n",
        "    obs_pca_red = pca.inverse_transform(obs_pca)\n",
        "    print(\"Now I'm doing the minimum subtraction...\")\n",
        "    print(\"...removing the first %d principal components\"%(n_comp))\n",
        "    obs_pca_red = input_map - obs_pca_red\n",
        "\n",
        "    # get the array indices in the RING formulation\n",
        "    inds = np.arange(hp.nside2npix(MAP_NSIDE))\n",
        "    # transfer these to what they would be in the NESTED formulation\n",
        "    inds_nest = hp.ring2nest(MAP_NSIDE,inds)\n",
        "\n",
        "    sig = obs_pca_red\n",
        "\n",
        "    for PIX_SELEC in np.arange(hp.nside2npix(WINDOW_NSIDE)):\n",
        "        # get the indices of the pixels which actually are in the larger pixel\n",
        "        inds_in = np.where((inds_nest//NPIX_WINDOW)==PIX_SELEC)\n",
        "        to_rearr_inds = inds_nest[inds_in] - PIX_SELEC*NPIX_WINDOW\n",
        "        to_rearr = obs_pca_red[inds_in]\n",
        "        to_rearr = (to_rearr[np.argsort(to_rearr_inds)])[rearr]\n",
        "        to_rearr = np.reshape(to_rearr,(WINDOW_LENGTH,WINDOW_LENGTH,n_nu_out))\n",
        "        ind = (0)*nwinds + PIX_SELEC\n",
        "        pca_reduced_out[ind] = to_rearr\n",
        "\n",
        "    return pca_reduced_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh1LmsxHGZB2"
      },
      "source": [
        "Now that we've loaded our data in and defined our function, let's perform the PCA preprocessing, one sky at a time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lK2FoED9sia"
      },
      "source": [
        "%%time\n",
        "n_side = 4   # data arrangement parameter\n",
        "n_voxels = hp.nside2npix(4)\n",
        "print('we have %d voxels per sky simulation'%(n_voxels))\n",
        "\n",
        "splt = obs_map.shape[0]//n_voxels\n",
        "print(\"we\\'re working with %d sky simulations\"%(splt))\n",
        "pca3 = []\n",
        "\n",
        "for i,sky in enumerate(np.split(obs_map, splt, axis=0)):\n",
        "# do the PCA subtraction for each map separately\n",
        "    print('starting PCA subtraction for sky %d'%(i+1))\n",
        "    pca3.append(pca_subtraction(sky, 3, index_arr, n_nu=64,\n",
        "                    nu_arr=nu_arr, nu_start=0, n_nu_out=64, n_nu_avg=3))\n",
        "\n",
        "pca3 = np.concatenate(pca3, axis=0)\n",
        "print('I put the PCA-3 subtraction put back into this shape : ', pca3.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUhT9fj0o2z1"
      },
      "source": [
        "# build the UNet model\n",
        "The UNet module used for this analysis is built using Keras (with Tensorflow backend). Keras is a really intuitive deep learning library, but still requires lots of hyperparameter tuning. We'll declare a dictionary of parameters to get the UNet up and running.\n",
        "\n",
        "**Note:** this toy model we're building is much simpler than the one used in the publication. For the full `deep21` implementation we opted for a depth of 6 (halving input voxels $6$ times). To deal with the large number of weights to train, we parallelized training over 4 GPUs in parallel to handle the large number of parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gb8zjEoqC6w"
      },
      "source": [
        "from unet import unet_3d\n",
        "# DEFINE UNET INPUT PARAMS\n",
        "params = {\n",
        "    'nu_dim'        : 64,\n",
        "    'x_dim'         : 64,\n",
        "    'n_filters'     : 32,\n",
        "    'conv_width'    : 3,\n",
        "    'network_depth' : 3,\n",
        "    'batch_size'    : 16,\n",
        "    'num_epochs'    : 50,\n",
        "    'act'           : 'relu',\n",
        "    'lr'            : 0.0002,\n",
        "    'wd'            : 1e-5,\n",
        "    'batchnorm_in'  : True,\n",
        "    'batchnorm_out' : True,\n",
        "    'batchnorm_up'  : True,\n",
        "    'batchnorm_down': True,\n",
        "    'momentum'      :  0.02,\n",
        "    'model_num'     : 1,\n",
        "    'load_model'    : False,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSNm5zAsqfwb"
      },
      "source": [
        "net = unet_3d.unet3D(n_filters=params['n_filters'],\n",
        "                      conv_width=params['conv_width'],\n",
        "                      nu_dim=params['nu_dim'],\n",
        "                      x_dim=params['x_dim'],\n",
        "                      network_depth=params['network_depth'],\n",
        "                      batchnorm_down=params['batchnorm_down'],\n",
        "                      batchnorm_in=params['batchnorm_in'],\n",
        "                      batchnorm_out=params['batchnorm_out'],\n",
        "                      batchnorm_up=params['batchnorm_up'],\n",
        "                      momentum=params['momentum']\n",
        "                      )\n",
        "net = net.build_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKOn02GPrEW4"
      },
      "source": [
        "net.compile(optimizer=tf.optimizers.Adam(learning_rate=params['lr'],\n",
        "                                                 beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
        "                                                 loss=\"logcosh\",metrics=[\"mse\", \"logcosh\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv e"
      ],
      "metadata": {
        "id": "hhupjQqFfcOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axClmCFahE6M"
      },
      "source": [
        "# take a look at what our model looks like with either of these two methods:\n",
        "from keras.utils import plot_model\n",
        "plot_model(net, dpi=36, show_shapes=True) # change dpi parameter to \"zoom in\"\n",
        "# net.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiTPtQXmhnJg"
      },
      "source": [
        "# train the UNet\n",
        "Now we'll split our single simulation into training, testing, and validation sets. In our actual `deep21` experiment, we split 100's of *simulations* into these sets, but for simplicity we'll stick to the $5\\times192$ voxels for this sample simulation.\n",
        "\n",
        "Furthermore, in the publication we implemented an augmented training scheme, namely each full-sky simulation was subject to\n",
        "\n",
        "1.   Randomly-generated Gaussian observational noise\n",
        "2.   Random rotations on the sphere\n",
        "\n",
        "We found that this enabled the network to learn to distinguish foregrounds from cosmological signal on scales larger than our input voxel size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfcpAUrXd3vx"
      },
      "source": [
        "# for a quicker run using a single simulation,\n",
        "# uncomment and use this train-test-split:\n",
        "\n",
        "x = np.expand_dims(pca3, axis=-1)\n",
        "y = np.expand_dims(cosmo, axis=-1)\n",
        "\n",
        "N_TRAIN = int(0.8*192)\n",
        "N_VAL  = int(0.1*192)\n",
        "\n",
        "x_train = x[:N_TRAIN]\n",
        "x_val   = x[N_TRAIN:-N_VAL]\n",
        "x_test  = x[-N_VAL:]\n",
        "\n",
        "y_train = y[:N_TRAIN]\n",
        "y_val   = y[N_TRAIN:-N_VAL]\n",
        "y_test  = y[-N_VAL:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syRRVMOtKVsX"
      },
      "source": [
        "# # put data into a train-validation-test split\n",
        "# # 3 train, 1 val, 1 test skies\n",
        "\n",
        "# x = np.expand_dims(pca3, axis=-1)\n",
        "# y = np.expand_dims(cosmo, axis=-1)\n",
        "\n",
        "# N_TRAIN = int(3*192)\n",
        "# N_VAL  = int(1*192)\n",
        "\n",
        "# x_train = x[:N_TRAIN]\n",
        "# x_val   = x[N_TRAIN:-N_VAL]\n",
        "# x_test  = x[-N_VAL:]\n",
        "\n",
        "# y_train = y[:N_TRAIN]\n",
        "# y_val   = y[N_TRAIN:-N_VAL]\n",
        "# y_test  = y[-N_VAL:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v38gU1JciDzf"
      },
      "source": [
        "Next we'll fit the UNet to the training data, checking the validation set after every epoch (pass through the whole dataset). We'll train for just 10 epochs and see how our network performs on the test data.\n",
        "\n",
        "**Note:** if you're running this for all 4 training and validation simulations, this could take a while"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGUtRkw3M5Nu"
      },
      "source": [
        "history = net.fit(x_train, y_train, batch_size=4, epochs=10,\n",
        "                  validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUk2wvTTulLY"
      },
      "source": [
        "# save model weights !\n",
        "import os\n",
        "os.mkdir('./model')\n",
        "net.save_weights('./model/tutorial_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yir88iJhQmFA"
      },
      "source": [
        "# plot parameters\n",
        "plt.figure(figsize=(12,7))\n",
        "colors = ['#2c0342', '#223167', '#286d87', '#4fb49d', '#9af486']\n",
        "#labels = [r'$x_0 = $' + str(x0) for x0 in x0_arr]\n",
        "\n",
        "# plot training history and validation scores\n",
        "plt.plot(history.history['loss'], color=colors[0],\n",
        "         label='train loss')\n",
        "plt.plot(history.history['val_loss'], color=colors[2], linestyle='--',\n",
        "         label='validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('epoch #')\n",
        "plt.ylabel('LogCosh loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0mQ8XVgb0OI"
      },
      "source": [
        "# make prediction & plot voxel slices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMHRN_yNO_HV"
      },
      "source": [
        "# make prediction\n",
        "y_pred = net.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyqq36iyWJ6g"
      },
      "source": [
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "# actual frequencies\n",
        "nutable = './sim_info/nuTable.txt'\n",
        "(bn,nu_bot,nu_top,z_bot,z_top) = np.loadtxt(nutable).T\n",
        "nu_arr = ((nu_bot + nu_top)/2.)[:-1]\n",
        "\n",
        "# which frequencies we want to plot\n",
        "nu_indx = [0, 20, 60]\n",
        "for nu in nu_indx:\n",
        "  pick = 5\n",
        "\n",
        "  xval_c1 = np.squeeze(x_test)[pick].T\n",
        "  yval_c1 = np.squeeze(y_test)[pick].transpose()\n",
        "  y_c1_pred = np.squeeze(y_pred)[pick].transpose()\n",
        "\n",
        "  fig,axs = plt.subplots(constrained_layout=True)\n",
        "\n",
        "  ax1 = plt.subplot(131)\n",
        "  im = ax1.imshow(yval_c1[nu], vmin=0, rasterized=True,\n",
        "                  interpolation='spline16')\n",
        "\n",
        "  # create an axes on the right side of ax. The width of cax will be 5%\n",
        "  # of ax and the padding between cax and ax will be fixed at 0.05 inch.\n",
        "  divider = make_axes_locatable(ax1)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "  cbar = plt.colorbar(im, cax=cax)\n",
        "  cbar.set_label(r'$T_b\\ \\rm [mK]$')\n",
        "  ax1.set_xticks([])\n",
        "  ax1.set_yticks([])\n",
        "  ax1.set_title(r\"true cosmological signal\")\n",
        "\n",
        "\n",
        "  ax1 = plt.subplot(132)\n",
        "  im = plt.imshow(xval_c1[nu], vmin=-1, rasterized=True,\n",
        "                  interpolation='spline16')\n",
        "\n",
        "  # create an axes on the right side of ax. The width of cax will be 5%\n",
        "  # of ax and the padding between cax and ax will be fixed at 0.05 inch.\n",
        "  divider = make_axes_locatable(ax1)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "  cbar = plt.colorbar(im, cax=cax)\n",
        "\n",
        "\n",
        "  cbar.set_label(r'$T_b\\ \\rm [mK]$')\n",
        "  ax1.set_xticks([])\n",
        "  ax1.set_yticks([])\n",
        "  ax1.set_title(r\"$\\rm PCA-3 $ input\")\n",
        "\n",
        "\n",
        "\n",
        "  ax1 = plt.subplot(133)\n",
        "  im = plt.imshow(y_c1_pred[nu], vmin=0, rasterized=True,\n",
        "                  interpolation='spline16')\n",
        "\n",
        "  divider = make_axes_locatable(ax1)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "  cbar = plt.colorbar(im, cax=cax)\n",
        "\n",
        "  cbar.set_label(r'$T_b\\ \\rm [mK]$')\n",
        "\n",
        "  ax1.set_xticks([])\n",
        "  ax1.set_yticks([])\n",
        "  ax1.set_title(r\"UNet reconstruction\")\n",
        "\n",
        "\n",
        "  plt.suptitle(r'$\\nu = $%03d'%(nu_arr[nu]) + r' $\\rm MHz$')\n",
        "\n",
        "  plt.gcf().set_size_inches((3.7* 3.37, 3.37))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ImhXyM2rtXr"
      },
      "source": [
        "So even with a few epochs (and only one full-sky simulation), we see that the UNet does a decent job of reconstructing the cosmological signal.\n",
        "___\n",
        "\n",
        "(We could alternatively load a pre-trained model and weights from the repository.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUCUIRIm_xvc"
      },
      "source": [
        "Let's see how well the UNet captures the temperature distribution over frequency compared with the PCA-3 inputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWD8gNyTY6Xa"
      },
      "source": [
        "# see how the mean temperatures line up\n",
        "plt.plot(nu_arr[:192][::3], [np.mean(t) for t in np.squeeze(x_test).T],\n",
        "         color=colors[0], label='PCA-3 input')\n",
        "\n",
        "plt.plot(nu_arr[:192][::3], [np.mean(t) for t in np.squeeze(y_pred).T],\n",
        "         color=colors[2], label='UNet prediction')\n",
        "plt.plot(nu_arr[:192][::3], [np.mean(t) for t in np.squeeze(y_test).T],\n",
        "         color = colors[3], linestyle=':', label='cosmo target')\n",
        "\n",
        "plt.ylabel(r'$\\langle T_b \\rangle$')\n",
        "plt.xlabel(r'frequency $\\rm [MHz]$')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gt-0TiAZOJx"
      },
      "source": [
        "# power spectra\n",
        "An important way of quantifying cosmological maps is through clustering statistics obtained from maps such as power spectra. Here we'll take a look at angular clustering at a few different frequencies, first of the cleaning residual (how much is left over after we've implemented a cleaning algorithm) -- then we'll look at the bias introduced by cleaning on the target power spectrum\n",
        "\n",
        "**Note:** for this part of the tutorial, we need to have a prediction made *over an entire sky*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONlmkYL9_Ggr"
      },
      "source": [
        "def angularPowerSpec(y_true, prediction, nu_select,\n",
        "                     nu_arr, index_array, nu_avg=3):\n",
        "\n",
        "    rearr = index_array\n",
        "    nwinds = hp.nside2npix(4)\n",
        "\n",
        "    # true map\n",
        "    cosmo_test = (np.array_split(y_true, y_true.shape[0] // nwinds))\n",
        "\n",
        "    # cleaned map\n",
        "    y_pred = (np.array_split(prediction, prediction.shape[0] // nwinds))\n",
        "\n",
        "    # residual map\n",
        "    y_res = (np.array_split((prediction - y_true), y_true.shape[0] // nwinds))\n",
        "\n",
        "\n",
        "    cosmo_Cl = []   # Cls for cosmo spectra\n",
        "    pred_Cl  = []   # Cls for predicted spectra\n",
        "    res_Cl   = []   # Cls for residual spectra\n",
        "\n",
        "    for i,nu in enumerate(nu_select):\n",
        "\n",
        "        # Get Cls for COSMO spectrum\n",
        "        # loops over nsims test set skies\n",
        "        cos = []\n",
        "        for cosmo in cosmo_test:\n",
        "            cosmo0 = (cosmo.T[nu].T).flatten()\n",
        "            cosmo0 = cosmo0[rearr]\n",
        "            alm_cosmo = hp.map2alm(cosmo0)\n",
        "            Cl_cosmo = hp.alm2cl(alm_cosmo)\n",
        "            cos.append(Cl_cosmo)\n",
        "\n",
        "        # save average of Cl over nsims\n",
        "        cosmo_Cl.append(np.mean(cos, axis=0))\n",
        "\n",
        "\n",
        "        # Get Cls for the predicted maps\n",
        "        predicted_cl = []\n",
        "        for y in y_pred:\n",
        "            y0 = (y.T[nu].T).flatten()\n",
        "            y0 = y0[rearr]\n",
        "            alm_y = hp.map2alm(y0)\n",
        "            Cl_y = hp.alm2cl(alm_y)\n",
        "            predicted_cl.append(Cl_y)\n",
        "\n",
        "        # save average of Cl over nsims\n",
        "        pred_Cl.append(np.mean(predicted_cl, axis=0))\n",
        "\n",
        "\n",
        "        # Get Cls for the residual maps\n",
        "        residual_cl = []\n",
        "        for y in y_res:\n",
        "            y0 = (y.T[nu].T).flatten()\n",
        "            y0 = y0[rearr]\n",
        "            alm_y = hp.map2alm(y0)\n",
        "            Cl_y = hp.alm2cl(alm_y)\n",
        "            residual_cl.append(Cl_y)\n",
        "\n",
        "        # save average of Cl over nsims\n",
        "        res_Cl.append(np.mean(residual_cl, axis=0))\n",
        "\n",
        "\n",
        "    return np.array(cosmo_Cl), np.array(pred_Cl), np.array(res_Cl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLOhAYm5UIvE"
      },
      "source": [
        "%%time\n",
        "#  squeeze out extra dims\n",
        "unet_prediction = np.squeeze(y_pred)\n",
        "cosmo_target = np.squeeze(y_test)\n",
        "pca3_prediction = np.squeeze(x_test)\n",
        "\n",
        "# which frequencies do we want to look at ? Let's do the same as the cutouts above\n",
        "nu_indx = [0, 20, 60]\n",
        "\n",
        "cosmo_cl, unet_cl, unet_res_cl = angularPowerSpec(cosmo_target, unet_prediction,\n",
        "                                                  nu_indx, nu_arr, index_arr)\n",
        "\n",
        "cosmo_cl, pca3_cl, pca3_res_cl = angularPowerSpec(cosmo_target, pca3_prediction,\n",
        "                                                  nu_indx, nu_arr, index_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8aeLk7i9-XH"
      },
      "source": [
        "## residual map statistics\n",
        "Just how clean are the maps we've produced via the UNet method ? To quantify this, we'll take a look at the $\\rho_{\\rm res}$ statistic we introduced in the `deep21` paper. We define the *residual map* as $\\textbf{p} - \\textbf{t}$, where $\\textbf{p}$ and $\\textbf{t}$ are prediction and target maps, respectively. The power spectrum of this residual is then\n",
        "$$ \\rho_{\\rm res} = \\frac{P(\\textbf{p} - \\textbf{t})}{P(\\textbf{t})} $$\n",
        "where $P(\\cdot)$ is the power spectrum for a given orientation. We'll take a look at the angular power spectrum here, computed via the `healpy` library. The angular power spectrum decomposes a sky map (at a given radio frequency) into spherical harmonics, with $\\ell$ indexing the scale of signal fluctuations. Small $\\ell$ corresponds to the largest scales, while large $\\ell$ indexes small angular correlations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3wCpRskXVR_"
      },
      "source": [
        "for i,nu in enumerate(nu_indx):\n",
        "    plt.plot(pca3_res_cl[i] / cosmo_cl[i],\n",
        "            c=colors[0], label='PCA-3')\n",
        "    plt.plot(unet_res_cl[i] / cosmo_cl[i],\n",
        "            c=colors[3], label='UNet')\n",
        "\n",
        "    plt.yscale('log')\n",
        "    plt.ylabel(r'$\\rho_{\\rm res}(\\ell)$')\n",
        "\n",
        "    if i == len(nu_indx)-1:\n",
        "        plt.xlabel(r'(large scales) $\\leftarrow$     $\\ell$    $\\rightarrow$  (small scales)')\n",
        "\n",
        "    else:\n",
        "        plt.xlabel(r'$\\ell$')\n",
        "\n",
        "    plt.suptitle(r'$\\nu = $%03d'%(nu_arr[nu]) + r' $\\rm MHz$')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScnhGWenPGhO"
      },
      "source": [
        "## summary statistic bias\n",
        "\n",
        "We've demonstrated that in this noise-free, simple example that our maps appear qualitatively similar to the target cosmological signal. Many current cosmological analyses (references ?), however  rely on compressed summary statistics derived from compressed maps. In the publication we introduced the metric\n",
        "$$ \\varepsilon(k) = \\frac{P(\\textbf{p}) - P(\\textbf{t})}{P(\\textbf{t})} $$\n",
        "\n",
        "which quantifies the *bias* a map-cleaning method imposes on the derived summary statistic\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEXFOqXfYGui"
      },
      "source": [
        "for i,nu in enumerate(nu_indx):\n",
        "\n",
        "    plt.plot(np.abs(pca3_res_cl[i] - cosmo_cl[i]) / cosmo_cl[i],\n",
        "             c=colors[0],label='PCA-3')\n",
        "    plt.plot(np.abs(unet_cl[i] - cosmo_cl[i]) / cosmo_cl[i],\n",
        "             c=colors[3],label='UNet')\n",
        "    plt.yscale('log')\n",
        "    plt.ylabel(r'$\\varepsilon(\\ell)$')\n",
        "\n",
        "    if i == len(nu_indx)-1:\n",
        "        plt.xlabel(r'(large scales) $\\leftarrow$     $\\ell$    $\\rightarrow$  (small scales)')\n",
        "\n",
        "    else:\n",
        "        plt.xlabel(r'$\\ell$')\n",
        "\n",
        "    plt.suptitle(r'$\\nu = $%03d'%(nu_arr[nu]) + r' $\\rm MHz$')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-pE5W-LsRec"
      },
      "source": [
        "Computing the this statistic reveals that the UNet very capably reduces bias on the expected power spectrum, hovering around 10% of the target spectrum at all three frequencies that we extracted from the test set. At higher frequencies, PCA-3 is more capable at resolving foregrounds and cosmological signal. However, the UNet is by far more consistent across frequency and at all angular scales.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp6eHPELZDBv"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}